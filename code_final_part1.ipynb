{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL pre processing\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer  \n",
    "\n",
    "#list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#initializing Snowball Stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        #lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "        \n",
    "        # removing special characters and punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        #removing numerical values\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        #replacing multiple whitespaces with a single whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        #removing leading and trailing whitespaces\n",
    "        text = text.strip()\n",
    "        \n",
    "        #removing stopwords\n",
    "        text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "        \n",
    "        #stemimg words using Snowball Stemmer\n",
    "        text = stem_text(text)\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        return ''  \n",
    "\n",
    "def expand_contractions(text):\n",
    "    #common English contractions\n",
    "    contractions_dict = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "    #finding contractions\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contractions_dict.get(match) if contractions_dict.get(match) else contractions_dict.get(match.lower())\n",
    "        return expanded_contraction\n",
    "\n",
    "    #replacing contractions with expansions\n",
    "    expanded_text = contractions_re.sub(expand_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "try:\n",
    "    chunks_list = []\n",
    "    chunk_size = 10000  \n",
    "    \n",
    "    \n",
    "    for chunk in pd.read_csv(r'enwiki-20170820.csv', chunksize=chunk_size, low_memory=False, encoding='utf-8'):    \n",
    "        chunk = chunk.drop_duplicates(subset='ARTICLE_ID', keep='first')\n",
    "        chunk['SECTION_TEXT'] = chunk['SECTION_TEXT'].apply(clean_text)\n",
    "        \n",
    "        #filtering out rows where ARTICLE_ID equals TITLE\n",
    "        chunk = chunk[chunk['ARTICLE_ID'] != chunk['TITLE']]\n",
    "        \n",
    "        chunks_list.append(chunk)\n",
    "    \n",
    "    df = pd.concat(chunks_list, ignore_index=True)\n",
    "    df.drop_duplicates(subset='ARTICLE_ID', keep='first', inplace=True)\n",
    "\n",
    "    #saving preprocessed data into file\n",
    "    df.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted vocabulary\n",
    "try:\n",
    "    df = pd.read_csv('preprocessed_data.csv')\n",
    "\n",
    "    # Define a function to tokenize the text\n",
    "    def tokenize_text(text):\n",
    "        if isinstance(text, str):\n",
    "            tokens = word_tokenize(text)\n",
    "            return tokens\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    # only tokenizingbn'SECTION_TEXT' column\n",
    "    df['TOKENIZED_SECTION_TEXT'] = df['SECTION_TEXT'].apply(tokenize_text)\n",
    "\n",
    "    vocabulary = set()\n",
    "    for tokens in df['TOKENIZED_SECTION_TEXT']:\n",
    "        vocabulary.update(tokens)\n",
    "#removing empty spaces\n",
    "    filtered_vocabulary = [word for word in vocabulary if word and word[0].isalpha()]\n",
    "    sorted_vocabulary = sorted(filtered_vocabulary)\n",
    "\n",
    "    print(\"\\nSorted Vocabulary:\", sorted_vocabulary)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing DataFrame:Â {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEXING\n",
    "\n",
    "indexed_vocabulary = [(index, word) for index, word in enumerate(sorted_vocabulary)]\n",
    "\n",
    "print(\"\\nIndexed and Sorted Vocabulary:\", indexed_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
