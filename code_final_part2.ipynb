{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   TOKENIZED_SECTION_TEXT  \\\n",
      "0       ['', 'Anarchism, '', ', is, a, political, phil...   \n",
      "1       ['', 'Autism, '', ', is, a, neurodevelopmental...   \n",
      "2       [Percentage, of, diffusely, reflected, sunligh...   \n",
      "3       [Writing, cursive, forms, of, A, '', ', A, '',...   \n",
      "4       ['', 'Alabama, '', ', (, ), is, a, state, in, ...   \n",
      "...                                                   ...   \n",
      "816930  [anglian, combination, football, league, engli...   \n",
      "816931  [national, theatre, oslo, one, norways, larges...   \n",
      "816932  [midnight, runner, novel, jack, higgins, publi...   \n",
      "816933  [thats, call, music, may, refer, least, two, d...   \n",
      "816934  [vergulde, draeck, gilt, dragon, metre, tonne,...   \n",
      "\n",
      "                                         TERM_FREQUENCIES  \n",
      "0       [(1168158, 3), (840112, 4), (1371987, 1), (136...  \n",
      "1       [(1168158, 10), (840112, 7), (1303422, 1), (10...  \n",
      "2       [(568848, 1), (1325372, 16), (1017460, 1), (14...  \n",
      "3       [(818457, 1), (995611, 1), (1080866, 2), (1325...  \n",
      "4       [(1168158, 12), (840112, 1), (1490666, 7), (11...  \n",
      "...                                                   ...  \n",
      "816930  [(868659, 4), (976279, 3), (1079558, 2), (1218...  \n",
      "816931  [(1297636, 1), (1525209, 3), (1335823, 1), (13...  \n",
      "816932  [(1269409, 1), (1430480, 1), (1318604, 2), (11...  \n",
      "816933  [(1524987, 4), (941283, 4), (1289874, 3), (125...  \n",
      "816934  [(1571372, 1), (1028538, 1), (1101037, 1), (10...  \n",
      "\n",
      "[816935 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#calculating term frequency\n",
    "from collections import Counter\n",
    "\n",
    "#term frequencies for a document\n",
    "def calculate_tf(tokens):\n",
    "    #count the frequency of each token in the document\n",
    "    tf = Counter(tokens)\n",
    "    #convert the counts to the format (term_index, frequency)\n",
    "    return [(indexed_vocabulary[word], freq) for word, freq in tf.items() if word in indexed_vocabulary]\n",
    "\n",
    "indexed_vocabulary = {word: index for index, word in enumerate(sorted_vocabulary)}\n",
    "#adding a colun for frequencies\n",
    "df['TERM_FREQUENCIES'] = df['TOKENIZED_SECTION_TEXT'].apply(calculate_tf)\n",
    "\n",
    "print(df[['TOKENIZED_SECTION_TEXT', 'TERM_FREQUENCIES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Term  Total_Term_Frequency\n",
      "0                is               1221085\n",
      "1                 a               1687613\n",
      "2         political                 34127\n",
      "3        philosophy                  6526\n",
      "4              that                314609\n",
      "...             ...                   ...\n",
      "1674119  hardnheavy                     1\n",
      "1674120      cnsobu                     1\n",
      "1674121   jorsalfar                     1\n",
      "1674122    vergulde                     1\n",
      "1674123      draeck                     1\n",
      "\n",
      "[1674124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#calculating idf\n",
    "from collections import defaultdict\n",
    "\n",
    "total_term_frequency = defaultdict(int)\n",
    "#iterating through each document and updating the count of each\n",
    "for tokens in df['TOKENIZED_SECTION_TEXT']:\n",
    "    for token in tokens:\n",
    "        if token in indexed_vocabulary:\n",
    "            total_term_frequency[token] += 1\n",
    "\n",
    "idf_df = pd.DataFrame(list(total_term_frequency.items()), columns=['Term', 'Total_Term_Frequency'])\n",
    "\n",
    "print(idf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   TOKENIZED_SECTION_TEXT  \\\n",
      "0       ['', 'Anarchism, '', ', is, a, political, phil...   \n",
      "1       ['', 'Autism, '', ', is, a, neurodevelopmental...   \n",
      "2       [Percentage, of, diffusely, reflected, sunligh...   \n",
      "3       [Writing, cursive, forms, of, A, '', ', A, '',...   \n",
      "4       ['', 'Alabama, '', ', (, ), is, a, state, in, ...   \n",
      "...                                                   ...   \n",
      "816930  [anglian, combination, football, league, engli...   \n",
      "816931  [national, theatre, oslo, one, norways, larges...   \n",
      "816932  [midnight, runner, novel, jack, higgins, publi...   \n",
      "816933  [thats, call, music, may, refer, least, two, d...   \n",
      "816934  [vergulde, draeck, gilt, dragon, metre, tonne,...   \n",
      "\n",
      "                                           TF_IDF_WEIGHTS  \n",
      "0       [(1168158, 1.7232534871538019), (840112, 1.772...  \n",
      "1       [(1168158, 5.744178290512672), (840112, 3.1023...  \n",
      "2       [(568848, 10.08695428669744), (1325372, 6.8397...  \n",
      "3       [(818457, 7.18682635385591), (995611, 8.940485...  \n",
      "4       [(1168158, 6.8930139486152076), (840112, 0.443...  \n",
      "...                                                   ...  \n",
      "816930  [(868659, 41.9112823815378), (976279, 15.57789...  \n",
      "816931  [(1297636, 3.1449396769129514), (1525209, 15.2...  \n",
      "816932  [(1269409, 7.297956809791266), (1430480, 6.592...  \n",
      "816933  [(1524987, 32.431933101524955), (941283, 20.40...  \n",
      "816934  [(1571372, 13.6133148113136), (1028538, 13.613...  \n",
      "\n",
      "[816935 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#calculating weights\n",
    "import math\n",
    "\n",
    "#alculating Document Frequencies (DF) for IDF calculation\n",
    "document_frequency = defaultdict(int)\n",
    "for tokens in df['TOKENIZED_SECTION_TEXT']:\n",
    "    unique_tokens = set(tokens)\n",
    "    for token in unique_tokens:\n",
    "        if token in indexed_vocabulary:\n",
    "            document_frequency[token] += 1\n",
    "\n",
    "#calculating IDF\n",
    "N = len(df)  # Total number of documents\n",
    "idf = {term: math.log(N / df_value) for term, df_value in document_frequency.items()}\n",
    "\n",
    "#calculating TF/IDF weights based on your original TF calculation method\n",
    "def calculate_tf_idf(term_frequencies):\n",
    "    # term_frequencies is a list of tuples (term_index, frequency) from your TF calculation\n",
    "    tf_idf_weights = [(term_index, freq * idf[indexed_vocabulary_inv[term_index]])\n",
    "                      for term_index, freq in term_frequencies\n",
    "                      if indexed_vocabulary_inv[term_index] in idf]\n",
    "    return tf_idf_weights\n",
    "\n",
    "#inverse mapping\n",
    "indexed_vocabulary_inv = {v: k for k, v in indexed_vocabulary.items()}\n",
    "\n",
    "#applying calculation\n",
    "df['TF_IDF_WEIGHTS'] = df['TERM_FREQUENCIES'].apply(calculate_tf_idf)\n",
    "\n",
    "\n",
    "print(df[['TOKENIZED_SECTION_TEXT', 'TF_IDF_WEIGHTS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from collections import defaultdict\\nimport pandas as pd\\nimport math\\n\\n# Initialize a dictionary to count the document frequency of each term\\ndocument_frequency = defaultdict(int)\\n\\n# Calculate document frequency for each term\\nfor tokens in df[\\'TOKENIZED_SECTION_TEXT\\']:\\n    unique_tokens = set(tokens)\\n    for token in unique_tokens:\\n        if token in indexed_vocabulary:\\n            document_frequency[token] += 1\\n\\n# Calculate IDF values\\nN = len(df)  # Total number of documents\\nidf = {term: math.log(N / df_value) for term, df_value in document_frequency.items()}\\n\\n# User Query and Relevance Calculation\\nuser_query = \"information retrieval model\"\\nquery_tokens = user_query.split()\\n\\n# Calculate TF-IDF for query\\ntf_idf_query = [(indexed_vocabulary[word], query_tokens.count(word) * idf[word]) for word in query_tokens if word in indexed_vocabulary]\\n\\n# Calculate relevance scores for each document\\nrelevance_scores = []\\nfor idx, row in df.iterrows():\\n    tf_idf_doc = row[\\'TF_IDF_WEIGHTS\\']\\n    similarity_score = calculate_cosine_similarity(tf_idf_doc, tf_idf_query)\\n    relevance_scores.append((row[\\'ARTICLE_ID\\'], similarity_score))\\n\\n# Sort documents by relevance score\\nrelevance_scores = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\\n\\n# Print top k relevant documents\\ntop_k = 5\\nprint(f\"Top {top_k} Relevant Documents for Query: \\'{user_query}\\':\")\\nfor doc_id, score in relevance_scores[:top_k]:\\n    print(f\"Document ID: {doc_id}, Relevance Score: {score}\")'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uses cosine to calculate similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# # Initialize a dictionary to count the document frequency of each term\n",
    "# document_frequency = defaultdict(int)\n",
    "\n",
    "# # Calculate document frequency for each term\n",
    "# for tokens in df['TOKENIZED_SECTION_TEXT']:\n",
    "#     unique_tokens = set(tokens)\n",
    "#     for token in unique_tokens:\n",
    "#         if token in indexed_vocabulary:\n",
    "#             document_frequency[token] += 1\n",
    "\n",
    "# Calculate IDF values\n",
    "N = len(df)  # Total number of documents\n",
    "idf = {term: math.log(N / df_value) for term, df_value in document_frequency.items()}\n",
    "\n",
    "# User Query and Relevance Calculation, using hardcoded query\n",
    "user_query = \"information retrieval model\"\n",
    "query_tokens = user_query.split()\n",
    "\n",
    "# Calculate TF-IDF for query\n",
    "tf_idf_query = [(indexed_vocabulary[word], query_tokens.count(word) * idf[word]) for word in query_tokens if word in indexed_vocabulary]\n",
    "\n",
    "# Calculate relevance scores for each document\n",
    "relevance_scores = []\n",
    "for idx, row in df.iterrows():\n",
    "    tf_idf_doc = row['TF_IDF_WEIGHTS']\n",
    "    similarity_score = calculate_cosine_similarity(tf_idf_doc, tf_idf_query)\n",
    "    relevance_scores.append((row['ARTICLE_ID'], similarity_score))\n",
    "\n",
    "# Sort documents by relevance score\n",
    "relevance_scores = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top k relevant documents\n",
    "top_k = 5\n",
    "print(f\"Top {top_k} Relevant Documents for Query: '{user_query}':\")\n",
    "for doc_id, score in relevance_scores[:top_k]:\n",
    "    print(f\"Document ID: {doc_id}, Relevance Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Hardcoded user query\\nuser_query = \"information retrieval model\"\\n\\n# Tokenize the user query\\nquery_tokens = user_query.split()\\n\\n# Calculate TF/IDF weights for the query\\ntf_idf_query = [(indexed_vocabulary[word], query_tokens.count(word) * idf[word]) for word in query_tokens if word in indexed_vocabulary]\\n\\n# Function to create a vector representation of a document/query in the vector space model\\ndef create_vector(tf_idf_weights, vocabulary_size):\\n    vector = [0] * vocabulary_size  # Initialize vector with zeros\\n    for term_index, weight in tf_idf_weights:\\n        vector[term_index] = weight\\n    return vector\\n\\n# Create vectors for the query and documents\\nquery_vector = create_vector(tf_idf_query, len(indexed_vocabulary))\\ndocument_vectors = [create_vector(doc[\\'TF_IDF_WEIGHTS\\'], len(indexed_vocabulary)) for _, doc in df.iterrows()]\\n\\n# Calculate relevance scores\\nrelevance_scores = []\\nfor doc_id, document_vector in enumerate(document_vectors):\\n    # Calculate the inner product (scalar product) of the query vector with the document vector\\n    inner_product = sum(query_weight * document_weight for query_weight, document_weight in zip(query_vector, document_vector))\\n    relevance_scores.append((doc_id, inner_product))\\n\\n# Sort relevance scores in descending order\\nrelevance_scores = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\\n\\n# Print the top k relevant documents\\ntop_k = 5\\nprint(f\"Top {top_k} Relevant Documents for Query: \\'{user_query}\\':\")\\nfor doc_id, score in relevance_scores[:top_k]:\\n    print(f\"Document ID: {doc_id}, Relevance Score: {score}\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takes too long to run\n",
    "\n",
    "\n",
    "# Hardcoded user query\n",
    "user_query = \"information retrieval model\"\n",
    "\n",
    "# Tokenize the user query\n",
    "query_tokens = user_query.split()\n",
    "\n",
    "# Calculate TF/IDF weights for the query\n",
    "tf_idf_query = [(indexed_vocabulary[word], query_tokens.count(word) * idf[word]) for word in query_tokens if word in indexed_vocabulary]\n",
    "\n",
    "# Function to create a vector representation of a document/query in the vector space model\n",
    "def create_vector(tf_idf_weights, vocabulary_size):\n",
    "    vector = [0] * vocabulary_size  #initializing vector with zero\n",
    "    for term_index, weight in tf_idf_weights:\n",
    "        vector[term_index] = weight\n",
    "    return vector\n",
    "\n",
    "# Create vectors for the query and documents\n",
    "query_vector = create_vector(tf_idf_query, len(indexed_vocabulary))\n",
    "document_vectors = [create_vector(doc['TF_IDF_WEIGHTS'], len(indexed_vocabulary)) for _, doc in df.iterrows()]\n",
    "\n",
    "# Calculate relevance scores\n",
    "relevance_scores = []\n",
    "for doc_id, document_vector in enumerate(document_vectors):\n",
    "    # Calculate the inner product (scalar product) of the query vector with the document vector\n",
    "    inner_product = sum(query_weight * document_weight for query_weight, document_weight in zip(query_vector, document_vector))\n",
    "    relevance_scores.append((doc_id, inner_product))\n",
    "\n",
    "# Sort relevance scores in descending order\n",
    "relevance_scores = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top k relevant documents\n",
    "top_k = 5\n",
    "print(f\"Top {top_k} Relevant Documents for Query: '{user_query}':\")\n",
    "for doc_id, score in relevance_scores[:top_k]:\n",
    "    print(f\"Document ID: {doc_id}, Relevance Score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
